{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Links to datasets</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/datasets/mrajaxnp/cert-insider-threat-detection-research/data?select=http.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imported Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "\n",
    "# NLP Tools\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Models and Metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingClassifier, IsolationForest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report, mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Download required NLTK data (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pre Pre-processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('test.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['attachments'] = dataset['attachments'].apply(lambda x: x.count(';') + 1 if pd.notna(x) else 0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns='activity', axis=1)\n",
    "dataset = dataset.head(1000)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pre-processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'datetime' column into 'date' and 'time' columns\n",
    "split_df = dataset['date'].str.split(' ', expand=True)\n",
    "dataset['date'] = split_df[0]\n",
    "dataset['time'] = split_df[1].fillna('00:00:00')  # Fill missing time values with a default time if needed\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert email addresses to the number of addresses in each row\n",
    "dataset['to'] = dataset['to'].str.count(';') + 1\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cc addresses to the number of addresses in each row\n",
    "dataset['cc'] = dataset['cc'].apply(lambda x: x.count(';') + 1 if pd.notna(x) else 0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bcc addresses to the number of addresses in each row\n",
    "dataset['bcc'] = dataset['bcc'].apply(lambda x: x.count(';') + 1 if pd.notna(x) else 0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text in 'from' to lowercase\n",
    "dataset['from'] = dataset['from'].str.lower()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "dataset = dataset[['id', 'pc', 'user', 'date', 'time', 'from', 'to', 'cc', 'bcc', 'size', 'attachments', 'content']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '/' from the 'date'\n",
    "dataset['date'] = dataset['date'].str.replace('/', '')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ':' from the 'time'\n",
    "dataset['time'] = dataset['time'].str.replace(':', '')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"PC-\" prefix\n",
    "dataset['pc'] = dataset['pc'].str.replace('PC-', '')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "dataset = dataset[['id', 'user', 'from', 'pc','date', 'time', 'to', 'cc', 'bcc', 'size', 'attachments', 'content']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Count\n",
    "dataset['char_count'] = dataset['content'].apply(len)\n",
    "\n",
    "# Word Count\n",
    "dataset['word_count'] = dataset['content'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Average Word Length\n",
    "dataset['avg_word_length'] = dataset['content'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "\n",
    "# Counting the number of sentences\n",
    "dataset['sentence_count'] = dataset['content'].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "dataset = dataset[['id', 'user', 'from', 'pc','date', 'time', 'to', 'cc', 'bcc', 'size', 'attachments', 'char_count', 'word_count', 'avg_word_length', 'sentence_count', 'content']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column 'pc', 'date' and time to int64\n",
    "dataset['pc'] = dataset['pc'].astype('int64')\n",
    "\n",
    "dataset['date'] = dataset['date'].astype('int64')\n",
    "\n",
    "dataset['time'] = dataset['time'].astype('int64')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Standardize numeric columns\n",
    "scaler = StandardScaler()\n",
    "dataset[numeric_columns] = scaler.fit_transform(dataset[numeric_columns])\n",
    "dataset.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Identify numerical anomalies</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores\n",
    "z_scores = stats.zscore(dataset.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# Get boolean DataFrame indicating the presence of anomalies (using a threshold of 2)\n",
    "anomalies_bool_df = pd.DataFrame(z_scores, columns=dataset.select_dtypes(include=['float64', 'int64']).columns, index=dataset.index)\n",
    "anomalies_bool_df = (anomalies_bool_df > 2) | (anomalies_bool_df < -2)\n",
    "\n",
    "# Extract anomalies\n",
    "anomalies = dataset[anomalies_bool_df.any(axis=1)]\n",
    "\n",
    "# Drop anomalies from the original dataset\n",
    "df_no_anomalies = dataset.drop(anomalies.index)\n",
    "\n",
    "# Display the datasets\n",
    "print(\"Normal Dataset:\")\n",
    "print(df_no_anomalies)\n",
    "print(\"\\nAnomalous Dataset:\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_anomalies['numeric_anomalies'] = 0\n",
    "anomalies['numeric_anomalies'] = 1\n",
    "# Combine dataframes vertically\n",
    "dataset = pd.concat([df_no_anomalies, anomalies])\n",
    "\n",
    "# Order by index\n",
    "dataset = dataset.sort_index()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Pre-processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text in 'content' to lowercase\n",
    "dataset['content'] = dataset['content'].str.lower()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_email_content(email):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(email)\n",
    "\n",
    "    # Removal of stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the 'content' column\n",
    "dataset['content'] = dataset['content'].apply(preprocess_email_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Identifify Sentiment Anomalies</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis and get polarity scores\n",
    "dataset['sentiment_score'] = dataset['content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Detect anomalies using Isolation Forest\n",
    "clf = IsolationForest(contamination=0.2)\n",
    "dataset['sentiment_anomaly'] = clf.fit_predict(dataset[['sentiment_score']])\n",
    "\n",
    "# Create a color map based on the 'sentiment_anomaly' values\n",
    "colors = dataset['sentiment_anomaly'].apply(lambda x: 'red' if x == -1 else 'green')\n",
    "\n",
    "# Plot sentiment scores and anomalies\n",
    "plt.scatter(dataset.index, dataset['sentiment_score'], c=colors)\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.title('Anomaly Detection in Sentiment Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all occurrences of 1 with 0 in 'sentiment_anomaly' and then all occurrences of -1 with 1 in the same column.\n",
    "dataset['sentiment_anomaly'] = dataset['sentiment_anomaly'].replace(1, 0)\n",
    "dataset['sentiment_anomaly'] = dataset['sentiment_anomaly'].replace(-1, 1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'target' column\n",
    "dataset['target'] = dataset['numeric_anomalies'] + dataset['sentiment_anomaly']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_columns = ['numeric_anomalies', 'sentiment_score', 'sentiment_anomaly']\n",
    "\n",
    "# Standardize numeric columns\n",
    "scaler = StandardScaler()\n",
    "dataset[numeric_columns] = scaler.fit_transform(dataset[numeric_columns])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "dataset = dataset[['id', 'user', 'from', 'pc','date', 'time', 'to', 'cc', 'bcc', 'size', 'attachments', 'char_count', 'word_count', 'avg_word_length', 'sentence_count', 'sentiment_score', 'numeric_anomalies', 'sentiment_anomaly','content', 'target']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Selection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = dataset.corr()\n",
    "# Generate a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[['pc','date', 'time', 'char_count', 'word_count', 'avg_word_length', 'sentence_count', 'sentiment_score', 'numeric_anomalies', 'sentiment_anomaly', 'target']]\n",
    "dataset = dataset[['pc','date', 'time', 'to', 'cc', 'bcc', 'size', 'attachments', 'char_count', 'word_count', 'avg_word_length', 'sentence_count', 'sentiment_score', 'numeric_anomalies', 'sentiment_anomaly', 'target']]\n",
    "X = dataset.drop('target', axis=1)  # Features\n",
    "y = dataset['target']  # Target variable (0 for normal, 1 for insider threat)\n",
    "feature_names = X.columns  # This is the corrected line\n",
    "\n",
    "# 1. Filter Method\n",
    "# Removing features with high correlation\n",
    "correlation_matrix = pd.DataFrame(X, columns=feature_names).abs()\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "features_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] < 0.8)]\n",
    "X_filtered = pd.DataFrame(X, columns=feature_names).drop(features_to_drop, axis=1)\n",
    "\n",
    "# 2. Wrapper Method\n",
    "# Recursive Feature Elimination\n",
    "estimator = LinearRegression()\n",
    "selector = RFE(estimator=estimator, n_features_to_select=5, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "selected_features_wrapper = np.array(feature_names)[selector.support_]\n",
    "\n",
    "# 3. Embedded Method\n",
    "# Feature importances from Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "importances = model.feature_importances_\n",
    "selected_features_embedded = np.array(feature_names)[importances > np.mean(importances)]\n",
    "\n",
    "print(\"Features selected by Filter method:\", list(X_filtered.columns))\n",
    "print(\"Features selected by Wrapper method:\", list(selected_features_wrapper))\n",
    "print(\"Features selected by Embedded method:\", list(selected_features_embedded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['pc', 'time', 'cc', 'bcc', 'size', 'sentiment_score', 'word_count', 'avg_word_length', 'target']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = dataset.corr()\n",
    "\n",
    "# Generate a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model training and testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = dataset.drop('target', axis=1)  # Features\n",
    "y = dataset['target']  # Target variable (0 for normal, 1 for insider threat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # split ration 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Print a classification report for more detailed evaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM classifier without class weights\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Use average='weighted' for multiclass\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')          # Use average='weighted' for multiclass\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"SVM Accuracy:\", accuracy)\n",
    "print(\"SVM Recall:\", recall)\n",
    "print(\"SVM F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier without class weights\n",
    "logistic_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall_per_class = recall_score(y_test, y_pred, average='weighted')  # Removed average='weighted' and set to None\n",
    "f1_per_class = f1_score(y_test, y_pred, average='weighted')          # Removed average='weighted' and set to None\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Logistic Regression Accuracy:\", accuracy)\n",
    "print(\"Logistic Regression Recall per class:\", recall_per_class)\n",
    "print(\"Logistic Regression F1-score per class:\", f1_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree classifier without class weights\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall_per_class = recall_score(y_test, y_pred, average='weighted')  # Removed average='weighted' and set to None\n",
    "f1_per_class = f1_score(y_test, y_pred, average='weighted')          # Removed average='weighted' and set to None\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Decision Tree Accuracy:\", accuracy)\n",
    "print(\"Decision Tree Recall per class:\", recall_per_class)\n",
    "print(\"Decision Tree F1-score per class:\", f1_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual classifiers without class weights\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "svc_classifier = SVC(kernel='linear', C=1.0, probability=True, random_state=42)\n",
    "logistic_classifier = LogisticRegression(random_state=42)\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create an ensemble classifier using voting\n",
    "ensemble_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', random_forest),\n",
    "    ('svc', svc_classifier),\n",
    "    ('logistic', logistic_classifier),\n",
    "    ('tree', tree_classifier)\n",
    "], voting='soft')  # Use 'soft' voting for probability-based voting\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ensemble_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall_per_class = recall_score(y_test, y_pred, average='weighted')  # Changed to average=None\n",
    "f1_per_class = f1_score(y_test, y_pred, average='weighted')          # Changed to average=None\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n",
    "print(\"Ensemble Recall per class:\", recall_per_class)\n",
    "print(\"Ensemble F1-score per class:\", f1_per_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
